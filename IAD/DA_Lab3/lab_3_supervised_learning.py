# -*- coding: utf-8 -*-
"""Lab_3.Supervised_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sBmkLoUb1QsqofkQ8sQTvdZsZ-JvT9OO

# <center>Майнор "Интеллектуальный анализ данных"</center>

# <center>Курс "Введение в анализ данных"</center>

# <center>Лабораторная работа №3. Supervised Learning</center>

## Данные

В рамках данной лабораторной работы вам предлагается проанализировать набор данных о студентах двух школ в Португалии.  
В файле `students_data.csv` представлена информация о студентах, посещающих два курса - математику (`Math`) и поргутальский язык (`Por`). Некоторые студенты представлены в обоих курсах, некоторые - только в одном. Для каждого студента известны три оценки по курсу: оценка за первое полугодие (`G1`), оценка за второе полугодие (`G2`) и итоговая оценка за год (`G3`).
"""

!pip install graphviz
!pip install wand
!apt-get install libmagickwand-dev
!pip install xgboost

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import matplotlib.pyplot as plt
import pprint as pp
from tabulate import tabulate as tb
import scipy.stats as stats
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder
import numpy as np

def print_dict(d: dict, key_label = 'Key', item_label = 'Item'):
    print(tb(d.items(), headers = [key_label, item_label]))

pd.set_option('display.max_columns', 40)

pd.set_option('display.max_colwidth', None)

data = pd.read_csv("students_data.csv")
data.shape

data.head(15)

"""### Признаки

Данные представлены признаками различных типов: числовыми, категориальными, упорядоченными категориальными.

**Описание признаков:**
"""

pd.read_csv('students_data_features.csv',
            delimiter=';',
            encoding='windows-1251')

"""## Часть 1. Предобработка данных

* Разделите данные на две части - данные для моделирования (80%) и отложенную выборку (20%). Убедитесь, что распределение целевой переменной (`G3`) одинаково в обоих частях.  
  __NB__: Отложенную выборку нужно использовать только для финальной оценки качества модели. Обучение и кросс-валидацию следует проводить на данных для моделирования.  
* Выполните необходимые преобразования данных: исправление ошибок, удаление выбросов и пропусков, приведение признаков к числовому виду.  
* Оцените значимость признаков для определения итоговой оценки за курс. Исключите из выборки незначимые на ваш взгляд признаки, обоснуйте свое решение. 
* (Опционально) Feature engineering: создайте новые признаки (значимые) на основе уже имеющихся.
  
**Tip:** Используйте свои наработки из Лабораторной работы №1.

### Преобразование данных

Проверить на ошибочные значения
"""

# Check mistakes in categorical values
def count_categories(data_frame=data, target_type = 'object', drop_id = True):
    categories = {label : set(data_frame[label].values) for label in data_frame.columns 
                  if data_frame[label].dtypes == target_type and not (drop_id and label == "ID")}
    for key in categories:
        categories[key] = (categories[key], len(categories[key]))
    # Let's watch what categorical values we have in each column
    return categories
# return categories
print_dict(count_categories())

# Columns with errors:
# sex ('m'), Pstatus ('t'), Medu and Fedu ('0' and strings instead int64)),
# Mjob and Fjob ('at-home' = 'at_home'), guardian ('futher'), 
# romantic and cheating have nan - I'll fix it later
#
data['sex'].replace('m', 'M', inplace=True)
data['Pstatus'].replace('t', 'T', inplace=True)
data['Medu'].replace('o', '0', inplace=True)
data['Fedu'].replace('o', '0', inplace=True)
data['Mjob'].replace('at-home', 'at_home', inplace=True)
data['Fjob'].replace('at-home', 'at_home', inplace=True)
data['guardian'].replace('futher', 'father', inplace=True)
print_dict(count_categories())

# Cast to int64
data['Medu'] = data['Medu'].apply(int, convert_dtype=True)
data['Fedu'] = data['Fedu'].apply(int, convert_dtype=True)
print("Medu:", data['Medu'].dtypes, ", Fedu:", data['Fedu'].dtypes)

"""Проверить на пропущенные значения"""

# Find columns with NaN values
nullInfo = data.isnull().sum()
nullInfo[nullInfo > 0]

print(data["romantic"].value_counts(dropna=False))
print(data["famrel"].value_counts(dropna=False))
print(data["Dalc"].value_counts(dropna=False))
print(data["Walc"].value_counts(dropna=False))
print(data["cheating"].value_counts(dropna=False))

# Column 'cheating' has too many NaN values, so it's better to drop it off the dataFrame
data = data.drop("cheating", axis=1)
data.head(1)

# Check are there rows with several NaN values (to drop them)
nanCounter = data.isnull().sum(axis=1)
len(nanCounter[nanCounter > 1])

# Count mean
print("famrel: ", data['famrel'].describe()['mean'])
print("Dalc: ", data['Dalc'].describe()['mean'])
print("Walc: ", data['Walc'].describe()['mean'])

# Replace NaN in dataframe with mean or neutral values
# *if we haven't information about relationships of a person, so there is none of them :D
data = data.fillna({'romantic': "no", 'famrel': 4.0, 'Dalc': 1.0, 'Walc': 2.0})

# Cast to int64 'categorical floats'
data['Walc'] = data['Walc'].apply(int, convert_dtype=True)
data['Dalc'] = data['Dalc'].apply(int, convert_dtype=True)
data['famrel'] = data['famrel'].apply(int, convert_dtype=True)

"""Основываясь на предыдующей работе, даатсет не содержит серьезных выбросов, способных крайне негативно повлиять на построение моделей

Категориальные признаки нужно привести к `One-Hot-Encoded` форме
"""

# Label encoding
object_labels = [label for label in data.columns if data[label].dtypes == 'object']
data_labels_enc = data[object_labels].copy()

cats_info = count_categories(data_labels_enc)
for label in data_labels_enc.columns:
    cats_info[label] = (tuple(cats_info[label][0]), cats_info[label][1])
    for i in range(cats_info[label][1]):
        data_labels_enc[label].replace(cats_info[label][0][i], i, inplace=True)

data_labels_enc.head(5)

# One hot encoding for several catigorical columns
enc = OneHotEncoder(handle_unknown='ignore')

ohenc_labels = ['Mjob', 'Fjob', 'guardian', 'reason']
ohenc_data = pd.DataFrame(enc.fit_transform(data_labels_enc[ohenc_labels]).toarray())
ohenc_data.columns = ['is_' + label + '_'+ str(cats_info[label][0][i]) 
                     for label in ohenc_labels
                     for i in range(cats_info[label][1])]
for label in ohenc_data.columns:
    ohenc_data[label] = ohenc_data[label].apply(int, convert_dtype=True)
ohenc_data.head(5)

# Drop one hot encoded columns and leave only binary
data_labels_enc = data_labels_enc.drop(ohenc_labels, axis=1)
# Rename columns binary categorical columns to understand what does '1' mean in cells
data_labels_enc.columns = ['is_' + label + '_'+ str(cats_info[label][0][1]) for label in data_labels_enc.columns]
data_labels_enc.head(5)

# Now we have DataFrame with one hot encoding for all categorical columns
data_object_encoded = data_labels_enc.join(ohenc_data)
data_object_encoded.head(5)

"""### Learning data

Ground Truth значения
"""

labels = data['G3']

"""Данные для обучения"""

learning_data = data.drop(columns=['ID', 'G2', 'G3'])
learning_data = learning_data.drop(columns=object_labels)
learning_data = learning_data.join(data_object_encoded)

print('Learning data features: ', learning_data.shape[1])
learning_data.head(10)

"""Без G1 признака"""

learning_data_noG1 = learning_data.drop(columns=['G1'])
print('Learning data features (no G1): ', learning_data_noG1.shape[1])

"""### Лишние признаки & Feature eng.

Посмотрим какие признаки сильно коррелируют между собой, ведь это может ухудшить результаты обучения
"""

plt.figure(figsize=(40, 20))
sns.heatmap(learning_data.corr(), annot=True, fmt=".2f", linewidths=.5)

"""Выведем признаки с высокой корреляцией"""

def find_high_cov(data, cov_level=0.4) -> dict:
    high_cov = dict()
    for c in range(len(data.columns)):
        for i in range(c + 1, len(data.columns)):
            label_c = data.columns[c]
            label_i = data.columns[i]
            value = data.loc[label_c, label_i]
            if abs(value) >= cov_level:
                high_cov[(label_c, label_i)] = value
    return high_cov
print_dict(find_high_cov(learning_data.corr()), 'Pair', 'Cov')

"""Многие из коррелирующих признаков - взаимоисключающие one-hot encoded признаки и их изменять не стоит  
***
Однако я обрачу внимание на взаимосвязь признаков **Medu** и **Fedu**, **Dalc** и **Walc**  
В данном случае эти 4 признака можно свести к 2м путем объединения:
* **Medu** + **Fedu** = **Pedu** (можно считать образование обоих родителей)
* **Walc** + **Dalc** = **Alc** (учитываем общее пристрастие к алкоголю)
"""

learning_data['Alc'] = learning_data.loc[:, ['Walc', 'Dalc']].sum(1)
learning_data['Pedu'] = learning_data.loc[:, ['Fedu', 'Medu']].sum(1)
learning_data = learning_data.drop(columns=['Walc', 'Dalc', 'Fedu', 'Medu'])
learning_data.head()

learning_data_noG1 = learning_data.drop(columns=['G1'])

"""### Split data"""

from sklearn.model_selection import train_test_split

x = learning_data
x_no_g1 = learning_data_noG1
y = labels

"""установлен `random_state` чтобы контролировать разбиение"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=13)
x_no_g1_train, x_no_g1_test, y_train, y_test = train_test_split(x_no_g1, y, test_size=0.2, random_state=13)

"""Убедимся в правильности разбиения (в частности соотвествии `y`)"""

x_train.head(5)

x_no_g1_train.head(5)

y_train.head(5)

"""Рассмотрим распределение G3 в выборках `y_train` и `y_test`"""

# libraries & dataset
import seaborn as sns
import matplotlib.pyplot as plt
# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) 
sns.set(style="darkgrid")
 
# plotting both distibutions on the same figure
fig = sns.kdeplot(y_train, shade=True, color="r")
fig = sns.kdeplot(y_test, shade=True, color="b")
plt.show()

"""Мы можем видеть, что распределение G3 в `y_train` и `y_test` выборках одинаково

### Scale data
"""

from sklearn.preprocessing import StandardScaler, LabelEncoder

data_scaler = StandardScaler()

x_train = data_scaler.fit_transform(x_train)
x_test = data_scaler.transform(x_test)

x_no_g1_train = data_scaler.fit_transform(x_no_g1_train)
x_no_g1_test = data_scaler.transform(x_no_g1_test)

print('Train shape: ', x_train.shape)
print('Example:')
x_train[0]

print('Test shape: ', x_test.shape)
print('Example:')
x_test[0]

"""## Часть 2. Регрессия

* Решите задачу регрессии: постройте модель, предсказывающую итоговую оценку, которую получит студент по предмету (`G3`). При решении задачи **нельзя** использовать признак `G2`.  
<br>  
* Для решения задачи примените следующие методы:  
  * Линейная регрессия + регуляризации  
  * Полиномиальная регрессия  
  * KNN  
  * Деревья решений, Random Forest  
  
  Для каждого метода выполните настройку гиперпараметров на кросс-валидации.  
<br>    
* Оцените качество каждой модели на отложенной выборке, используйте различные метрики. Сравните модели и сделайте вывод о качестве решения задачи.  
<br>    
* Задачу необходимо решить в двух вариантах: с использованием признака `G1`  и без него. Сравните качество решений в двух случаях.  
<br>    
* В регрессионных моделях попробуйте дать интерпретацию весам признаков.
"""

from sklearn import metrics

def reg_scores(predicted, labels, printable=False):
  scores = dict()
  scores['mse'] = metrics.mean_squared_error(labels, predicted)
  scores['rmse'] = np.sqrt(scores['mse'])
  scores['mae'] = metrics.mean_absolute_error(labels, predicted)
  scores['medae'] = metrics.median_absolute_error(labels, predicted)
  scores['R2'] = metrics.r2_score(labels, predicted)
  # print
  if printable:
    scores_table = [(key, value) for key, value in scores.items()]
    print('SCORES:')
    print(tb(scores_table, headers=('metric', 'score'), tablefmt="fancy_grid"))
  return scores

def reg_scores_compare(predicted: dict, labels: dict, printable=True):
  scores_train = reg_scores(predicted['train'], labels['train'])
  scores_test = reg_scores(predicted['test'], labels['test'])
  # print
  if printable:
    keys = scores_train.keys()
    train_values = scores_train.values()
    test_values = scores_test.values()
    scores_table = [(key, train, test) for key, train, test in 
                    zip(keys, train_values, test_values)]
    print('SCORES:')
    print(tb(scores_table, 
             headers=('metric', 'train scores', 'test scores'), 
             tablefmt="fancy_grid"))            
  return scores_train, scores_test

def create_table_from_series(series, table_width=4):
  row_counter = 0
  row = []
  table = []
  for i in range(series.size):
    row.append(i)
    row.append(series.index[i])
    row.append("{:.3f}".format(series[i]))

    row_counter += 1
    if (row_counter + 1) % (table_width + 1) == 0:
      table.append(row)
      row = []
      row_counter = 0
  return table

def create_columns_from_series(series, table_width=4):
  row_counter = 0
  row = []
  table = []
  for i in range(series.size):
    row.append(i)
    row.append(series.index[i])
    row.append("{:.3f}".format(series[i]))

    row_counter += 1
    if (row_counter + 1) % (table_width + 1) == 0:
      table.append(row)
      row = []
      row_counter = 0
  if len(row):
    for i in range((table_width - row_counter) * 2):
      row.append(' ')
    table.append(row)
  return table

def reg_weights(model, columns, printable=True, table_width=4):
  weights = pd.Series(model.coef_, index=columns)
  sorted = abs(weights).sort_values(ascending=False)
  weights = weights[sorted.index]
  # print
  table = create_table_from_series(weights, table_width)
  if printable:
    print('WEIGHTS')
    print(weights)
  return weights

reg_test_scores = dict()
reg_test_scores_nog1 = dict()

"""### Linear regression"""

from sklearn import linear_model

"""#### Losso CV"""

losso_model = linear_model.LassoCV(alphas=np.arange(0.1, 30, 0.1), normalize=True, cv=5)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# losso_model.fit(x_train, y_train)
# print('Alpha: ', losso_model.alpha_)

"""Test"""

y_predicted_train = losso_model.predict(x_train)
y_predicted_test  = losso_model.predict(x_test)

_, reg_test_scores['losso'] = reg_scores_compare({'train': y_predicted_train, 
                                                  'test': y_predicted_test},
                                                 {'train': y_train, 
                                                  'test': y_test})

_ = reg_weights(losso_model, learning_data.columns)

"""Метрики у модели очень слабые, а по весам можно видеть, что предсказание делается, основываясь только на `G1` признаке - предыдущей оценке

No G1
"""

losso_model_nog1 = linear_model.LassoCV(alphas=np.arange(0.1, 30, 0.1), normalize=True, cv=5)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# losso_model_nog1.fit(x_no_g1_train, y_train)
# print('Alpha: ', losso_model_nog1.alpha_)

"""Test"""

y_predicted_train = losso_model_nog1.predict(x_no_g1_train)
y_predicted_test  = losso_model_nog1.predict(x_no_g1_test)

_, reg_test_scores_nog1['losso'] = reg_scores_compare({'train': y_predicted_train, 
                                                       'test': y_predicted_test},
                                                      {'train': y_train, 
                                                       'test': y_test})

_ = reg_weights(losso_model_nog1, learning_data.drop(columns=['G1']).columns)

"""Убрав у модели `G1` признак мы вовсе лешили ее возможности предсказывать - все веса равны нулю

#### Ridge CV
"""

ridge_model = ridgeModel = linear_model.RidgeCV(alphas=np.arange(0, 30, 0.1), 
                                                cv=5, 
                                                scoring='neg_root_mean_squared_error')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ridge_model.fit(x_train, y_train)
# print('Alpha: ', ridge_model.alpha_)

"""Test"""

y_predicted_train = ridge_model.predict(x_train)
y_predicted_test  = ridge_model.predict(x_test)

_, reg_test_scores['ridge'] = reg_scores_compare({'train': y_predicted_train, 
                                                  'test': y_predicted_test},
                                                 {'train': y_train, 
                                                  'test': y_test})

_ = reg_weights(ridge_model, learning_data.columns)

"""Данная линейная модель с Ridge регуляризацией показала хорошие результаты, судя по метрикам.  

Также по весам можно понять, что модель использует все параметры для предсказания в разной мере.  
Наиболее влияетельными оказались:
* предыдущая оценка `G1` (не удивительно)
* предмет `is_subject_Math` (отрицательный вес показывает, что по математике оценка должны быть ниже)
* прогулы `failures` (больше пропускаешь - оценка ниже)

No G1
"""

ridge_model_nog1 = ridgeModel = linear_model.RidgeCV(alphas=np.arange(0, 30, 0.1), 
                                                     cv=5, 
                                                     scoring='neg_root_mean_squared_error')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ridge_model_nog1.fit(x_no_g1_train, y_train)
# print('Alpha: ', ridge_model_nog1.alpha_)

"""Test"""

y_predicted_train = ridge_model_nog1.predict(x_no_g1_train)
y_predicted_test  = ridge_model_nog1.predict(x_no_g1_test)

_, reg_test_scores_nog1['ridge'] = reg_scores_compare({'train': y_predicted_train, 
                                                       'test': y_predicted_test},
                                                      {'train': y_train, 
                                                       'test': y_test})

_ = reg_weights(ridge_model_nog1, learning_data.drop(columns=['G1']).columns)

"""Без признака `G1` метрики стали сильно хуже, а большее влияение получили:
* `failures`
* `is_higher_yes`
* `studytime`

### Polynomial
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

params = {
    'polynomialfeatures__degree': [1, 2, 3] 
}

five_fold = KFold(n_splits=5, shuffle=True)
reg_model = linear_model.LinearRegression(normalize=True)
poly_scaler = PolynomialFeatures(include_bias=False)

polynomial_pipeline = make_pipeline(poly_scaler, reg_model)

poly_grid = GridSearchCV(polynomial_pipeline, params, cv=five_fold,
                  scoring='neg_mean_squared_error', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# poly_grid.fit(x_train, y_train)

"""Best hyperparams"""

poly_grid.best_params_

polynomial_model = poly_grid.best_estimator_

"""Test"""

y_predicted_train = polynomial_model.predict(x_train)
y_predicted_test  = polynomial_model.predict(x_test)

_, reg_test_scores['poly'] = reg_scores_compare({'train': y_predicted_train, 
                                                  'test': y_predicted_test},
                                                 {'train': y_train, 
                                                  'test': y_test})

"""Давайте убедимся в том, что выбранная степень полинома оптимальна.  
Поставим `degree=3` принудительно и сравним метрики
"""

poly_scaler.degree = 3
polynomial_model = make_pipeline(poly_scaler, reg_model)
polynomial_model.fit(x_train, y_train)

y_predicted_train = polynomial_model.predict(x_train)
y_predicted_test  = polynomial_model.predict(x_test)

_ = reg_scores_compare({'train': y_predicted_train, 
                        'test': y_predicted_test},
                       {'train': y_train, 
                        'test': y_test})

"""На метриках мы видим переобучение модели:  
* на `x_train` выборке все метрики показывает минимально возможную ошибку, значит наша модель смогла построить функцию, чтобы "попасть" во все точки датасета
* на `x_test` выборке модель показывает плохую обобщающую способность, по крайней мере хуже обычной линейной модели

No G1
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# poly_grid.fit(x_no_g1_train, y_train)

poly_grid.best_params_

polynomial_model_nog1 = poly_grid.best_estimator_

"""Test"""

y_predicted_train = polynomial_model_nog1.predict(x_no_g1_train)
y_predicted_test  = polynomial_model_nog1.predict(x_no_g1_test)

_, reg_test_scores_nog1['poly'] = reg_scores_compare({'train': y_predicted_train, 
                                                      'test': y_predicted_test},
                                                     {'train': y_train, 
                                                      'test': y_test})

"""Метрики сильно хуже моделей с `G1` признаком

Давайте убедимся в том, что выбранная степень полинома оптимальна.  
Поставим `degree=3` принудительно и сравним метрики
"""

poly_scaler.degree = 3
polynomial_model_nog1 = make_pipeline(poly_scaler, reg_model)
polynomial_model_nog1.fit(x_no_g1_train, y_train)

y_predicted_train = polynomial_model_nog1.predict(x_no_g1_train)
y_predicted_test  = polynomial_model_nog1.predict(x_no_g1_test)

_, reg_test_scores_nog1['poly'] = reg_scores_compare({'train': y_predicted_train, 
                                                      'test': y_predicted_test},
                                                     {'train': y_train, 
                                                      'test': y_test})

"""### KNN"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

params = {
    'kneighborsregressor__n_neighbors': np.arange(1, 50, 1), 
    'kneighborsregressor__weights': ['uniform', 'distance'],
    'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute']
}

five_fold = KFold(n_splits=5, shuffle=True)
knn_reg = KNeighborsRegressor()
knn_pipeline = make_pipeline(knn_reg)

knn_grid = GridSearchCV(knn_pipeline, params, cv=five_fold, 
                        scoring='neg_mean_absolute_error', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# knn_grid.fit(x_train, y_train)

knn_grid.best_params_

knn_model = knn_grid.best_estimator_

"""Test"""

y_predicted_train = knn_model.predict(x_train)
y_predicted_test  = knn_model.predict(x_test)

_, reg_test_scores['knn'] = reg_scores_compare({'train': y_predicted_train, 
                                                'test': y_predicted_test},
                                               {'train': y_train, 
                                                'test': y_test})

"""Так как KNN просто "запоминает" `train` выборку, то и метрики на ней наилучшие.  
Однако на *отложенных* данных видим, что обобщающая способность далеко не лучшая

No G1
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# knn_grid.fit(x_no_g1_train, y_train)

knn_grid.best_params_

knn_model_nog1 = knn_grid.best_estimator_

"""Test"""

y_predicted_train = knn_model_nog1.predict(x_no_g1_train)
y_predicted_test  = knn_model_nog1.predict(x_no_g1_test)

_, reg_test_scores_nog1['knn'] = reg_scores_compare({'train': y_predicted_train, 
                                                     'test': y_predicted_test},
                                                    {'train': y_train, 
                                                     'test': y_test})

"""Без `G1` параметра ожидаемо хуже  
Также можно заметить, что `grid.best_params_` такие же как и у моделей, обучаемых на выборке со всеми признаками

### Random Forest
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

params = {
    'randomforestregressor__n_estimators': np.arange(150, 250, 1), 
    # до этого сделал выборку 1 - 250 c шагом 5
    # оценил лучший подобранный параметр
    # и сейчас сузил выборку для подбора лушчего значения
    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2']
}

five_fold = KFold(n_splits=5, shuffle=True)
rf_reg = RandomForestRegressor()
rf_pipeline = make_pipeline(rf_reg)

rf_grid = GridSearchCV(rf_pipeline, params, cv=five_fold, 
                       scoring='neg_mean_absolute_error', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rf_grid.fit(x_train, y_train)

rf_grid.best_params_

rf_model = rf_grid.best_estimator_

y_predicted_train = rf_model.predict(x_train)
y_predicted_test  = rf_model.predict(x_test)

_, reg_test_scores['random forest'] = reg_scores_compare({'train': y_predicted_train, 
                                                          'test': y_predicted_test},
                                                         {'train': y_train, 
                                                          'test': y_test})

"""Мы видим довольно сильное падение в качестве модели на `test` выборке  
Можно поробовать избавиться от этого подобрав `max_depth` деревьев
"""

params = {
    'randomforestregressor__max_depth': np.arange(2, 50, 2)
}

five_fold = KFold(n_splits=5, shuffle=True)
rf_reg = RandomForestRegressor(n_estimators=218)
rf_pipeline = make_pipeline(rf_reg)

rf_grid = GridSearchCV(rf_pipeline, params, cv=five_fold, 
                       scoring='neg_mean_absolute_error', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rf_grid.fit(x_train, y_train)

rf_grid.best_params_

rf_model = rf_grid.best_estimator_

y_predicted_train = rf_model.predict(x_train)
y_predicted_test  = rf_model.predict(x_test)

_, reg_test_scores['random forest'] = reg_scores_compare({'train': y_predicted_train, 
                                                          'test': y_predicted_test},
                                                         {'train': y_train, 
                                                          'test': y_test})

"""Получилось немного улучшить метрики

**No G1**
"""

params = {
    'randomforestregressor__n_estimators': np.arange(150, 250, 2), 
    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2']
}

five_fold = KFold(n_splits=5, shuffle=True)
rf_reg = RandomForestRegressor()
rf_pipeline = make_pipeline(rf_reg)

rf_grid = GridSearchCV(rf_pipeline, params, cv=five_fold, 
                       scoring='neg_mean_absolute_error', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rf_grid.fit(x_no_g1_train, y_train)

rf_grid.best_params_

rf_model_nog1 = rf_grid.best_estimator_

"""Test"""

y_predicted_train = rf_model_nog1.predict(x_no_g1_train)
y_predicted_test  = rf_model_nog1.predict(x_no_g1_test)

_, reg_test_scores_nog1['random forest'] = \
                            reg_scores_compare({'train': y_predicted_train, 
                                                'test': y_predicted_test},
                                               {'train': y_train, 
                                                'test': y_test})

"""`RandomForestRegressor`, обученный на данных без признака `G1` имеет плохую обобщающую способность"""

params = {
    'randomforestregressor__max_depth': np.arange(2, 50, 2)
}

five_fold = KFold(n_splits=5, shuffle=True)
rf_reg = RandomForestRegressor(n_estimators=218)
rf_pipeline = make_pipeline(rf_reg)

rf_grid = GridSearchCV(rf_pipeline, params, cv=five_fold, 
                       scoring='neg_mean_absolute_error', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rf_grid.fit(x_no_g1_train, y_train)

rf_grid.best_params_

rf_model_nog1 = rf_grid.best_estimator_

"""Test"""

y_predicted_train = rf_model_nog1.predict(x_no_g1_train)
y_predicted_test  = rf_model_nog1.predict(x_no_g1_test)

_ = reg_scores_compare({'train': y_predicted_train, 
                        'test': y_predicted_test},
                       {'train': y_train, 
                        'test': y_test})

"""Улучшить результаты не получилось

### Results
"""

def compare_estimators(scores: dict, printable=True):
  estimators_names = list(scores.keys())
  headers = ['estimators'] + list(scores[estimators_names[0]].keys())
  table = []

  for estimator in estimators_names:
    row = [estimator]
    for metric_key in scores[estimator].keys():
      row.append(scores[estimator][metric_key])
    table.append(row)

  # print
  print(tb(table, headers=headers, tablefmt='fancy_grid'))

  return table

"""#### with G1"""

reg_test_scores['random forest']

_ = compare_estimators(reg_test_scores)

"""**Best Regression (G1)**  
***
*model*: **RandomForestRegressor**  
*params*: n_estimators=218, max_depth=6  
*score (R2)*: 0.716 
***

#### no G1
"""

_ = compare_estimators(reg_test_scores_nog1)

"""**Best Regression (no G1)**  
***
*model*: **KNeighborsRegressor**  
*params*: n_neighbors=26, algorithm="ball_tree", weights="distance"  
*score (R2)*: 0.16
***

## Часть 3. Бинарная классификация

Решите задачу бинарной классификации: постройте модель, предсказывающую, сдаст студент предмет (`G3` >= 8) или не сдаст (`G3` < 8). <br>При решении задачи **нельзя** использовать признаки `G1` и `G2`.

Уберем из использования G1
"""

x_train = x_no_g1_train
x_test  = x_no_g1_test

"""Создадим новый признак для предсказания"""

y_binary_train = (y_train >= 8).apply(int, convert_dtype=True)
y_binary_test  = (y_test >= 8).apply(int, convert_dtype=True)
y_binary_train

"""Проверим распредление"""

print('Train:')
print(y_binary_train.value_counts(normalize=True))
print('Test:') 
print(y_binary_test.value_counts(normalize=True))

# libraries & dataset
import seaborn as sns
import matplotlib.pyplot as plt
# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) 
sns.set(style="darkgrid")
 
# plotting both distibutions on the same figure
fig = sns.kdeplot(y_binary_train, shade=True, color="r")
fig = sns.kdeplot(y_binary_test, shade=True, color="b")
plt.show()

"""Распределение практически одинаковое и не будет негавтино влиять на процесс обучения"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score

"""### Задание 1  
  
* Постройте дерево решений глубины 5 (остальные параметры по умолчанию), оцените качество на 5-fold валидации.  
* Для одного из деревьев (т.е. обученного на одной из итераций кросс-валидации) выведите само дерево - постройте график или выведите в текстовом виде. По структуре дерева сформулируйте правила, по которым принимается решение.  
* Сравните между собой деревья решений, полученных на различных итерациях 5-fold валидации. Сделайте вывод, насколько сильно они похожи или различаются между собой.

5-fold validation
"""

five_fold = StratifiedKFold(n_splits=5, shuffle=True)
tree_models = []
tree_scores = []

for train_index, test_index in five_fold.split(x_train, y_train):
    # cv train 
    train_x = x_train[train_index]
    train_y = y_binary_train.iloc[train_index]
    # cv test
    test_x = x_train[test_index]
    test_y = y_binary_train.iloc[test_index]
    # model
    model = DecisionTreeClassifier(max_depth=5)
    model.fit(train_x, train_y)
    tree_models.append(model)
    # score
    # pred_y = model.predict(test_x)
    score = model.score(test_x, test_y)
    tree_scores.append(score)

print("CV MEAN SCORE: ", np.mean(tree_scores))
print("All scores: ", ', '.join(list(map(lambda x: "{:.3f}".format(x), tree_scores))))

"""Visualize"""

import graphviz
from sklearn.tree import export_graphviz

dot_data = export_graphviz(tree_models[0], 
                           out_file=None, 
                           feature_names=learning_data.drop(columns=['G1']).columns, 
                           class_names=['failed','passed'],
                           filled=True, 
                           rounded=True, 
                           special_characters=True)

graph = graphviz.Source(dot_data) 
graph.render("decision_tree_example.pdf")

from wand.image import Image as WImage
img = WImage(filename="decision_tree_example.pdf")
img

"""Compare trees

### Задание 2  
  
На кросс-валидации (5-fold из 2 повторений) оцените, как меняется качество модели Random Forest с ростом числа деревьев (при дефолтных значениях остальных параметров). Провизуализируйте результаты. Сколько деревьев достаточно в данном случае и почему?  
**NB:** В сравнение включите конфигурацию, аналогичную простому дереву решений.
"""

from sklearn import metrics

def bi_class_scores(predicted, labels, printable=False):
  scores = dict()
  scores['precision'] = metrics.precision_score(labels, predicted)
  scores['recall'] = metrics.recall_score(labels, predicted)                                                
  scores['f1'] = metrics.f1_score(labels, predicted)
  scores['accuracy'] = metrics.accuracy_score(labels, predicted)
  scores['roc-auc'] = metrics.roc_auc_score(labels, predicted)
  # print
  if printable:
    scores_table = [(key, value) for key, value in scores.items()]
    print('SCORES:')
    print(tb(scores_table, headers=('metric', 'score'), tablefmt="fancy_grid"))
  return scores

def bi_class_compare(predicted, labels, printable=False):
  pass

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier

# your code here

double_five_fold = RepeatedStratifiedKFold(n_splits=5, n_repeats=2)
# trees_test_numbers = [1, 5, 10, 15, 20, 40, 75, 100, 125, 150, 175, 200]
trees_test_numbers = np.arange(1, 50, 1)

table_rows = []

for trees_number in trees_test_numbers:
  scores = []
  for train_index, test_index in five_fold.split(x_train, y_binary_test):
      # cv train 
      train_x = x_train[train_index]
      train_y = y_binary_train.iloc[train_index]
      # cv test
      test_x = x_train[test_index]
      test_y = y_binary_train.iloc[test_index]
      # model
      model = RandomForestClassifier(n_estimators=trees_number)
      model.fit(train_x, train_y)
      # score
      pred_y = model.predict(test_x)
      scores.append(bi_class_scores(pred_y, test_y))

  mean_scores = {key: [] for key in scores[0].keys()}
  for example in scores:
    for key, value in example.items():
      mean_scores[key].append(value)
  
  for key in mean_scores.keys():
    mean_scores[key] = np.mean(mean_scores[key])
    # trees, score, metric
    row = (trees_number, mean_scores[key], key)
    table_rows.append(row)
  
trees_number_df = pd.DataFrame(table_rows, 
                               columns=['trees_number', 'score', 'metric'])
trees_number_df

plt.figure(figsize=(20, 10))
sns.lineplot(data=trees_number_df, x="trees_number", y="score", hue="metric")
plt.show()

"""***
**Вывод:**  
Основываясь на полученном графике оптимальное количество деревьев находится в районе 7-10
***

### Задание 3  
  
* Настройте гиперпараметры модели Random Forest на 5-fold валдиации. В качестве метрики используйте F1-score. Замерьте время, затраченное на вычисления.
* Обучите Random Forest  с настроенными параметрами на всех данных для моделирования. На отложенной выборке оцените качество (F1-score) всего ансамбля и <u>каждого дерева отдельно</u>. Постройте график распределения качества деревьев в ансамбле и сравните результаты с качеством всего леса. Дайте комментарий.  
* Выведите важность признаков в Random Forest, сделайте выводы.
"""

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

params = {
    'randomforestclassifier__n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 
                                             12, 15, 17, 20, 25, 40, 75, 100], 
    'randomforestclassifier__criterion': ['gini', 'entropy'], 
    'randomforestclassifier__max_depth': [2, 3, 4, 5, 10, 15 None]
}

five_fold = KFold(n_splits=5, shuffle=True)
rf_bi_class = RandomForestClassifier(random_state=13)
rf_bi_class_pipeline = make_pipeline(rf_bi_class)

rf_bi_class_grid = GridSearchCV(rf_bi_class_pipeline, params, cv=five_fold, 
                                scoring='f1', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rf_bi_class_grid.fit(x_train, y_binary_train)

rf_bi_class_grid.best_params_

rf_bi_class_grid.best_score_

rf_bi_class_model = RandomForestClassifier(criterion='entropy', n_estimators=20)
rf_bi_class_model.fit(x_train, y_binary_train)

y_predicted = rf_bi_class_model.predict(x_test)
print('F1: ', metrics.f1_score(y_binary_test, y_predicted))
print(classification_report(y_binary_test, y_predicted))

trees_scores = [metrics.f1_score(y_binary_test, tree.predict(x_test)) 
                for tree in rf_bi_class_model.estimators_]

trees_scores = pd.Series(trees_scores)
trees_scores

# libraries & dataset
import seaborn as sns
import matplotlib.pyplot as plt
# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) 
sns.set(style="darkgrid")
 
# plotting both distibutions on the same figure
fig = sns.kdeplot(trees_scores, shade=True, color="r")
plt.show()

"""***
**Вывод:**  
f1 score ансамбля - `0.975`  
f1 scores деревьев - большинство значений расположены в промежутке от `0.92-0.94`

Каждое дерево по отдельности предсказывает результат хуже, чем в ансамбле.
***

### Задание 4  
  
* Примените логистическую регрессию для решения задачи, подберите оптимальные значения гиперпараметров. Оцените качество (roc auc) на 5-fold валидации из 2 повторений. 
* Аналогично (на такой же валидации (тех же подвыборках) с такой же метрикой) оцените качество Random Forest  с подобранными в предыдущем задании параметрами. Сравните с качеством логистическом регрессии.
* Обучите логистическую модель с настроенными параметрами на всех данных для моделирования. На отложенной выборке оцените качество - постройте ROC-кривую, вычислите roc auc. Вычислите аналогичную метрику для Random Forest из Задания 3, сравните точность моделей.
"""

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

params = {
    'logisticregression__solver': ['newton-cg', 'lbfgs', 'saga', 'sag', 'liblinear'], 
    'logisticregression__class_weight': ['balanced', None],
    'logisticregression__multi_class': ['ovr', 'multinomial']
}

double_five_fold = RepeatedStratifiedKFold(n_splits=5, n_repeats=2)
logistic_bi_reg = LogisticRegression(max_iter=1000)
logistic_bi_pipeline = make_pipeline(logistic_bi_reg)

log_bi_reg_grid = GridSearchCV(logistic_bi_pipeline, params, cv=five_fold, 
                               scoring='roc_auc', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# log_bi_reg_grid.fit(x_train, y_binary_train)

log_bi_reg_grid.best_params_

log_bi_reg_grid.best_score_

"""#### Double 5-fold Validation

Logistic
"""

log_bi_reg_model = LogisticRegression(multi_class='ovr', solver='liblinear')

scores = cross_val_score(log_bi_reg_model, x_train, y_binary_train, 
                         cv=double_five_fold, 
                         scoring='roc_auc')
print(np.mean(scores))
scores

"""Random Forest"""

rf_bi_class_model = RandomForestClassifier(criterion='entropy', n_estimators=20)

scores = cross_val_score(rf_bi_class_model, x_train, y_binary_train, 
                         cv=double_five_fold, 
                         scoring='roc_auc')
print(np.mean(scores))
scores

"""Random Forest показывает результат немного лучше

#### Fully fitted
"""

log_bi_reg_model = LogisticRegression(multi_class='ovr', solver='liblinear')
_ = log_bi_reg_model.fit(x_train, y_binary_train)

rf_bi_class_model = RandomForestClassifier(criterion='entropy', n_estimators=20)
_ = rf_bi_class_model.fit(x_train, y_binary_train)

metrics.plot_roc_curve(log_bi_reg_model, x_test, y_binary_test)  
print('Logistic')
plt.show()

metrics.plot_roc_curve(rf_bi_class_model, x_test, y_binary_test)  
print('Random Forest')
plt.show()

"""***
**Вывод:**  
`Random Forest` покала себя лучше `Logistic Regression`  
как на 5-fold валидации так и во время теста на отложенной выборке

*RFC-ROC-AUC:* `0.87`  
*LR-ROC-AUC:* `0.78`

***

### Задание 5  
  
* Используйте для решения задачи один из фреймворков градиентного бустинга: XGBoost, LightGDB или CatBoost.  
* Оцените на 5-fold валидации, как растет качество модели на обучающей и на тестовой выборках при добавлении каждого дерева. Провизуализируйте результаты.  
* Настройте гиперпараметры модели на 5-fold валидации, в качестве метрики используйте F1-score. Замерьте время, затраченное на вычисления.  
* Обучите модель с настроенными параметрами на всех данных для моделирования и оцените качество на отложенной выборке. Сравните результаты с другими моделями, дайте комментарий.
"""

import xgboost as xgb
from xgboost.sklearn import XGBClassifier

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

"""#### Рост количества деревьев"""

five_fold = KFold(n_splits=5, shuffle=True)

trees_list = np.arange(1, 50, 1)

table_rows = []

for trees_number in trees_list:
  tree_scores = []
  for train_index, test_index in five_fold.split(x_train, y_train):
      # cv train 
      train_x = x_train[train_index]
      train_y = y_binary_train.iloc[train_index]
      # cv test
      test_x = x_train[test_index]
      test_y = y_binary_train.iloc[test_index]
      # model
      model = XGBClassifier(n_estimators=trees_number)
      model.fit(train_x, train_y)
      tree_models.append(model)
      # score
      pred_y = model.predict(test_x)
      score = metrics.f1_score(test_y, pred_y)
      tree_scores.append(score)
  mean_score = np.mean(tree_scores)
  row = (trees_number, mean_score)
  table_rows.append(row)

xgbc_df = pd.DataFrame(table_rows, columns=('trees_number', 'score'))
xgbc_df.head()

plt.figure(figsize=(10, 10))
sns.lineplot(data=xgbc_df, x="trees_number", y="score")
plt.show()

"""Значение `f1-score` практически не меняются от увеличения количества деревьев

#### Подбор параметров
"""

params = {
    'xgbclassifier__n_eta': np.arange(0, 1, 0.1), 
    'xgbclassifier__sampling_method': ['uniform', 'gradient_based'],
    'xgbclassifier__max_depth': np.arange(1, 15, 2),
    'xgbclassifier__gamma': [0, 1, 2, 5, 10],
}

five_fold = KFold(n_splits=5, shuffle=True)
xgbc_clas = XGBClassifier()
xgbc_pipeline = make_pipeline(xgbc_clas)

xgbc_grid = GridSearchCV(xgbc_pipeline, params, cv=five_fold, 
                         scoring='f1', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# xgbc_grid.fit(x_train, y_binary_train)

xgbc_grid.best_params_

xgbc_grid.best_score_

xgbc_model = XGBClassifier()
_ = xgbc_model.fit(x_train, y_binary_train)

y_predicted = xgbc_model.predict(x_test) 
print('F1 score:', metrics.f1_score(y_binary_test, y_predicted))

metrics.plot_roc_curve(xgbc_model, x_test, y_binary_test)  
print('XGBC')
plt.show()

print(classification_report(y_binary_test, y_predicted))

"""***
**Вывод:**  
`XGBClassifier` оказался лучше как `Random Forest`, так и `Logistic Regression`  
Модель хорошо предсказывает оба класса, несмотря на низкий support

**XGBC-ROC-AUC:** `0.89`  
**XGBC-F1:** `0.97`   

*RFC-ROC-AUC:* `0.87`  
*LR-ROC-AUC:* `0.78`

***

##  Часть 4. Многоклассовая классификация
  
* Решите задачу многоклассовой классификации: постройте модель, пресдказывающую оценку студента по предмету по 4 балльной шкале
    - Отлично: 18 <= `G3` <= 20
    - Хорошо: 14 <= `G3` <= 17
    - Удовлетворительно: 8 <= `G3` <= 13
    - Неудовлетворительно: `G3` < 8  
  
  При решении задачи **нельзя** использовать признаки `G1` и `G2`.  
  
  
* Для решения задачи примените следующие методы:  
  * KNN  
  * Логистическая регрессия  
  * Деревья решений  
  * Random Forest
  * Gradient Boosting
  
  На кросс-валидации подберите оптимальные значения гиперпараметров алгоритмов.  
  
  
* Оцените качество моделей, используйте confusion matrix и производные от нее метрики. Сделайте выводы.

### Prepare labels

Целевая переменная будет представлена следующими классами:  
- 0 - Отлично
- 1 - Хорошо
- 2 - Удовлетворительно
- 3 - Неудовлетворительно
"""

replace_dict = dict()
for score in range(21):
  if score < 8:
    replace_dict[score] = 3
  elif 8 <= score <= 13:
    replace_dict[score] = 2
  elif 14 <= score <= 17:
    replace_dict[score] = 1
  else:
    replace_dict[score] = 0

', '.join([f"{key}: {value}" for key, value in replace_dict.items()])

y_train = y_train.replace(replace_dict)
y_train

y_test = y_test.replace(replace_dict)
y_test

"""Убираем `G1` признак"""

x_train = x_no_g1_train
x_test  = x_no_g1_test

"""Рассмотрим распределение G3 в выборках `y_train` и `y_test`"""

# libraries & dataset
import seaborn as sns
import matplotlib.pyplot as plt
# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) 
sns.set(style="darkgrid")
 
# plotting both distibutions on the same figure
fig = sns.kdeplot(y_train, shade=True, color="r")
fig = sns.kdeplot(y_test, shade=True, color="b")
plt.show()

"""Мы можем видеть, что распределение G3 в `y_train` и `y_test` выборках одинаково"""

def class_scores(predicted, labels, printable=False):
  scores = dict()
  scores['precision_macro'] = metrics.precision_score(labels, predicted, 
                                                      average='macro')
  scores['precision_weighted'] = metrics.precision_score(labels, predicted, 
                                                         average='weighted')
  scores['recall_macro'] = metrics.recall_score(labels, predicted, 
                                                average='macro')
  scores['recall_weighted'] = metrics.recall_score(labels, predicted, 
                                                   average='weighted')
  scores['f1_macro'] = metrics.f1_score(labels, predicted, 
                                        average='macro')
  scores['f1_weighted'] = metrics.f1_score(labels, predicted, 
                                           average='weighted')
  scores['accuracy'] = metrics.accuracy_score(labels, predicted)
  # print
  if printable:
    scores_table = [(key, value) for key, value in scores.items()]
    print('SCORES:')
    print(tb(scores_table, headers=('metric', 'score'), tablefmt="fancy_grid"))
  return scores

def class_scores_compare(predicted: dict, labels: dict, 
                         print_sk_report=True, printable=True):
  scores_train = class_scores(predicted['train'], labels['train'])
  scores_test  = class_scores(predicted['test'], labels['test'])
  # print
  if printable:
    if print_sk_report:
      print('TRAIN:')
      print(classification_report(labels['train'], predicted['train']))
      print('\nTEST:')
      print(classification_report(labels['test'], predicted['test']))
    else:
      keys = scores_train.keys()
      train_values = scores_train.values()
      test_values = scores_test.values()
      scores_table = [(key, train, test) for key, train, test in 
                      zip(keys, train_values, test_values)]
      print('SCORES:')
      print(tb(scores_table, 
              headers=('metric', 'train scores', 'test scores'), 
              tablefmt="fancy_grid"))      
      
  return scores_train, scores_test

class_test_scores = dict()

from sklearn.metrics import plot_confusion_matrix

"""### KNN"""

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

params = {
    'kneighborsclassifier__n_neighbors': np.arange(1, 50, 1), 
    'kneighborsclassifier__weights': ['uniform', 'distance'],
    'kneighborsclassifier__algorithm': ['ball_tree', 'kd_tree', 'brute']
}

five_fold = KFold(n_splits=5, shuffle=True)
knn_clas = KNeighborsClassifier()
knn_pipeline = make_pipeline(knn_clas)

knn_grid = GridSearchCV(knn_pipeline, params, cv=five_fold, 
                        scoring='f1_weighted', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# knn_grid.fit(x_train, y_train)

knn_grid.best_params_

knn_classifier = knn_grid.best_estimator_

y_predicted_train = knn_classifier.predict(x_train)
y_predicted_test  = knn_classifier.predict(x_test)

_, class_test_scores['knn'] = class_scores_compare({'train': y_predicted_train, 
                                                    'test': y_predicted_test},
                                                   {'train': y_train, 
                                                    'test': y_test})

plot_confusion_matrix(knn_classifier, x_test, y_test, cmap=plt.get_cmap('Blues'))
plt.show()

"""Так как `KNN` запоминает всю `train` выборку, то и оценки на ней будут максимальные.  

Если же смотреть на результаты предсказания на `test` выборке,  
то видна главная проблема - классы `0` и `1`, у которых низкий  
support (количество примеров, представлоенных в выборке),  
ожидаемо предсказываются хуже, из за чего портится общая оценка классификатора.

### Логистическая регрессия
"""

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

params = {
    'logisticregression__solver': ['newton-cg', 'lbfgs', 'saga', 'sag'], 
    'logisticregression__class_weight': ['balanced', None],
    'logisticregression__multi_class': ['ovr', 'multinomial']
}

five_fold = KFold(n_splits=5, shuffle=True)
logistic_reg = LogisticRegression(max_iter=1000)
logistic_pipeline = make_pipeline(logistic_reg)

log_reg_grid = GridSearchCV(logistic_pipeline, params, cv=five_fold, 
                            scoring='f1_weighted', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# log_reg_grid.fit(x_train, y_train)

log_reg_grid.best_params_

log_reg_model = log_reg_grid.best_estimator_

y_predicted_train = log_reg_model.predict(x_train)
y_predicted_test  = log_reg_model.predict(x_test)

_, class_test_scores['log-regression'] = class_scores_compare({'train': y_predicted_train, 
                                                                'test': y_predicted_test},
                                                               {'train': y_train, 
                                                                'test': y_test})

plot_confusion_matrix(log_reg_model, x_test, y_test, cmap=plt.get_cmap('Blues'))
plt.show()

"""Предсказания классов с низким support свойтсвенно также как и `KNN`, однако в этот раз класс `0` не предсказывается вовсе - все метрики равны нулю.

### Деревья решений
"""

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

params = {
    'decisiontreeclassifier__criterion': ['gini', 'entropy'], 
    'decisiontreeclassifier__splitter': ['best', 'random'],
    'decisiontreeclassifier__max_features': ['auto', 'sqrt', 'log2'],
    'decisiontreeclassifier__min_samples_split': np.arange(2, 10, 1),
    'decisiontreeclassifier__max_depth': [2, 3, 4, 5, 10, 12, 15, 17, 20, 30, None],
    'decisiontreeclassifier__random_state': [13]
}

five_fold = KFold(n_splits=5, shuffle=True)
dec_tree_class = DecisionTreeClassifier()
dec_tree_class_pipeline = make_pipeline(dec_tree_class)

dec_tree_class_grid = GridSearchCV(dec_tree_class_pipeline, params, cv=five_fold, 
                            scoring='f1_weighted', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# dec_tree_class_grid.fit(x_train, y_train)

dec_tree_class_grid.best_params_

dec_tree_model = dec_tree_class_grid.best_estimator_

y_predicted_train = dec_tree_model.predict(x_train)
y_predicted_test  = dec_tree_model.predict(x_test)

_, class_test_scores['decistion-tree'] = class_scores_compare({'train': y_predicted_train, 
                                                                'test': y_predicted_test},
                                                               {'train': y_train, 
                                                                'test': y_test})

plot_confusion_matrix(dec_tree_model, x_test, y_test, cmap=plt.get_cmap('Blues'))
plt.show()

"""Снова видим нулевые метрики для `0` класса

### Random Forest
"""

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

params = {
    # 'randomforestclassifier__criterion': ['gini', 'entropy'], 
    'randomforestclassifier__n_estimators': np.arange(50, 200, 5), 
    'randomforestclassifier__max_depth': [2, 3, 4, 5, 10, 15, 20, 30, 50, None],
    # 'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2'],
    # 'randomforestclassifier__min_samples_split': np.arange(2, 10, 1)
}

five_fold = KFold(n_splits=5, shuffle=True)
rf_class = RandomForestClassifier(random_state=13)
rf_class_pipeline = make_pipeline(rf_class)

rf_class_grid = GridSearchCV(rf_class_pipeline, params, cv=five_fold, 
                            scoring='f1_weighted', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rf_class_grid.fit(x_train, y_train)

rf_class_grid.best_params_

rf_class_model = rf_class_grid.best_estimator_

y_predicted_train = rf_class_model.predict(x_train)
y_predicted_test  = rf_class_model.predict(x_test)

_, class_test_scores['random-forest'] = class_scores_compare({'train': y_predicted_train, 
                                                              'test': y_predicted_test},
                                                             {'train': y_train, 
                                                              'test': y_test})

plot_confusion_matrix(rf_class_model, x_test, y_test, cmap=plt.get_cmap('Blues'))
plt.show()

"""Судя по оценкам, полученным на `train` выборке, модель переобучилась.  
Однако после эксперементов с гиперпараметрами выяснилось, что `testz метрики данной модели лучше.

### Gradient Boosting
"""

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

params = {
    'gradientboostingclassifier__criterion': ['mse', 'mae'], 
    'gradientboostingclassifier__n_estimators': np.arange(50, 200, 5), 
    'gradientboostingclassifier__max_features': ['auto', 'sqrt', 'log2'],
    'gradientboostingclassifier__loss': ['deviance', 'exponential']
}

five_fold = KFold(n_splits=5, shuffle=True)
gb_class = GradientBoostingClassifier(random_state=13)
gb_class_pipeline = make_pipeline(gb_class)

gb_class_grid = GridSearchCV(gb_class_pipeline, params, cv=five_fold, 
                            scoring='f1_weighted', n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# gb_class_grid.fit(x_train, y_train)

gb_class_grid.best_params_

gb_class_model = gb_class_grid.best_estimator_

y_predicted_train = gb_class_model.predict(x_train)
y_predicted_test  = gb_class_model.predict(x_test)

_, class_test_scores['gradient-boosting'] = class_scores_compare({'train': y_predicted_train, 
                                                              'test': y_predicted_test},
                                                             {'train': y_train, 
                                                              'test': y_test})

plot_confusion_matrix(gb_class_model, x_test, y_test, cmap=plt.get_cmap('Blues'))
plt.show()

"""### Results"""

def compare_estimators(scores: dict, printable=True):
  estimators_names = list(scores.keys())
  headers = ['estimators'] + list(scores[estimators_names[0]].keys())
  table = []

  for estimator in estimators_names:
    row = [estimator]
    for metric_key in scores[estimator].keys():
      row.append(scores[estimator][metric_key])
    table.append(row)

  # print
  print(tb(table, headers=headers, tablefmt='fancy_grid'))

  return table

_ = compare_estimators(class_test_scores)

"""**Best Multiclass Classifier**  



> *Выводы*:  
результаты обучения всех моделей сильно зависит от   
набора данных и распределения целевых значений в нем  
Способность предсказывать конкретный класс уменьшается с уменьшием  
встречающихся примеров (support), иногда вовсе до нуля




Среди обученных моделей я бы выделил `KNN` и `Random Forest`.  
Данные модели показали наилучшие метрики (accuracy + f1).  

Однако `KNN` с преимуществом над `RF` определяет нулевой класс с крайне низким support.


***
*model*: **RandomForestRegressor**  
*params*: n_estimators=65, max_depth=30 
*score (accuracy/f1)*: 0.67 / 0/64 
***
*model*: **KNeighborsClassifier**  
*params*: algorithm='ball_tree', n_neighbors=3, weights='distance'  
*score (accuracy/f1)*: 0.66 / 0/66
"""